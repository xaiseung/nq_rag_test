{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faiss load/HuggingFace 파이프라인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "from dpr_embedding import CustomEmbeddings\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = CustomEmbeddings(\"facebook/dpr-question_encoder-single-nq-base\", model_kwargs={\"device_map\": DEVICE})\n",
    "faiss = FAISS.load_local(\"./db/faiss\", embeddings=embedding, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map = DEVICE, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "llama.generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama.generation_config.max_length = 96\n",
    "pipeline = transformers.pipeline(\n",
    "    model = llama,\n",
    "    tokenizer = tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    device_map=DEVICE, max_new_tokens=96,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\",\n",
    "\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. For example:\\n\n",
    "###\n",
    "Question: when is the last episode of season 8 of the walking dead\n",
    "Context: The eighth season of The Walking Dead, an American post-apocalyptic horror television series on AMC, premiered on October 22, 2017, and concluded on April 15, 2018, consisting of 16 episodes.\n",
    "Answer: March 18, 2018\n",
    "Question: what is the name of the most important jewish text\n",
    "Context: Codes of Jewish law are written that are based on the responsa; the most important code, the Shulchan Aruch, largely determines Orthodox religious practice today. Jewish philosophy refers to the conjunction between serious study of philosophy and Jewish theology.\n",
    "Answer: the Shulchan Aruch\n",
    "###\n",
    "Keep the answer very concise as one phrase, rather than sentences or clauses.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "llm = HuggingFacePipeline(pipeline=pipeline,\n",
    "                          model_kwargs={\n",
    "                          \"eos_token_id\":terminators\n",
    "                          })\n",
    "\n",
    "retriever = faiss.as_retriever(search_kwargs={\"k\":4})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def llama_preprocessing(chatvalue: ChatPromptValue):\n",
    "    messages = chatvalue.to_messages()\n",
    "    chat = []\n",
    "    for msg in messages:\n",
    "        msg_dict = msg.dict()\n",
    "        chat.append({\"role\": msg_dict[\"type\"], \"content\": msg_dict[\"content\"]})\n",
    "    return tokenizer.apply_chat_template(chat,tokenize=False)\n",
    "\n",
    "def output_parser(ai_message) -> str:\n",
    "    eot_str = \"<|eot_id|>\"\n",
    "    eot_idx = ai_message.find(eot_str)\n",
    "    if eot_idx != -1:\n",
    "        ai_message = ai_message[eot_idx+len(eot_str):]\n",
    "    return ai_message\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llama_preprocessing\n",
    "    | llm\n",
    "    | output_parser # 프롬프트를 포함한 출력을 보고싶다면 이 부분을 주석처리\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug run\n",
    "async for chunk in rag_chain.astream_log(\n",
    "    \"Who is the Moses's brother?\", include_names=\"Docs\"\n",
    "):\n",
    "    print(\"-\"*40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and preprocessing NQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_raw_datasets = datasets.load_dataset(\"google-research-datasets/natural_questions\")\n",
    "\n",
    "new_datasets = nq_raw_datasets.filter(\n",
    "    lambda batch: [len(\"\".join([t for sa in d[\"short_answers\"] for t in sa[\"text\"] ]) )>0 for d in batch[\"annotations\"]],\n",
    "    batched=True)\n",
    "\n",
    "def preprocessing(data):\n",
    "    sa_list = data[\"annotations\"][\"short_answers\"]\n",
    "    answer = \"\"\n",
    "    for sa in sa_list:\n",
    "        if len(sa[\"text\"]) != 0:\n",
    "            answer = ', '.join(sa[\"text\"])\n",
    "            break\n",
    "    data[\"answer\"] = answer.strip()\n",
    "    return data\n",
    "\n",
    "new_datasets = new_datasets.map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG + general-purpose LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20,30):\n",
    "    example = new_datasets[\"validation\"][i]\n",
    "    question = example[\"question\"][\"text\"]\n",
    "    answer = example[\"answer\"]\n",
    "    print(\"=\"*5)\n",
    "    print(\"Q:\", question)\n",
    "    pred = rag_chain.invoke(question)\n",
    "    print(\"pred:\", pred.strip().strip(\"assistant\").strip(\"Assistant\").strip())\n",
    "    print(\"GT:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xais_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
